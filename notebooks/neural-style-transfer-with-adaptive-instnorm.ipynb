{"cells":[{"cell_type":"markdown","metadata":{"id":"ud24EThsJ5Nh"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:07.58514Z","iopub.status.busy":"2022-02-05T23:28:07.584907Z","iopub.status.idle":"2022-02-05T23:28:17.137923Z","shell.execute_reply":"2022-02-05T23:28:17.137091Z","shell.execute_reply.started":"2022-02-05T23:28:07.585075Z"},"id":"DQv5KY_LLaf5","outputId":"a2867670-47aa-4343-af7a-2ec14af806f4","trusted":true},"outputs":[],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:17.140436Z","iopub.status.busy":"2022-02-05T23:28:17.140153Z","iopub.status.idle":"2022-02-05T23:28:20.106313Z","shell.execute_reply":"2022-02-05T23:28:20.105567Z","shell.execute_reply.started":"2022-02-05T23:28:17.140399Z"},"id":"eOhvtnD5J49-","trusted":true},"outputs":[],"source":["import collections\n","import glob\n","import io\n","import os\n","import pickle\n","import random\n","import shutil\n","import time\n","import warnings\n","import zipfile\n","warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n","\n","import IPython\n","import IPython.display\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import skimage\n","import skimage.color\n","import skimage.io\n","import skimage.transform\n","\n","import tqdm.auto as tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import torchvision\n","\n","import torchinfo"]},{"cell_type":"markdown","metadata":{"id":"a20tbSYkLmHc"},"source":["# Setting random seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.10788Z","iopub.status.busy":"2022-02-05T23:28:20.107636Z","iopub.status.idle":"2022-02-05T23:28:20.116092Z","shell.execute_reply":"2022-02-05T23:28:20.115267Z","shell.execute_reply.started":"2022-02-05T23:28:20.107849Z"},"id":"mxq29ZwVLqJz","trusted":true},"outputs":[],"source":["RANDOM_STATE = 42\n","random.seed(RANDOM_STATE)\n","os.environ['PYTHONHASHSEED'] = str(RANDOM_STATE)\n","np.random.seed(RANDOM_STATE)\n","torch.manual_seed(RANDOM_STATE)\n","torch.cuda.manual_seed(RANDOM_STATE)\n","torch.cuda.manual_seed_all(RANDOM_STATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.118945Z","iopub.status.busy":"2022-02-05T23:28:20.118671Z","iopub.status.idle":"2022-02-05T23:28:20.123717Z","shell.execute_reply":"2022-02-05T23:28:20.122969Z","shell.execute_reply.started":"2022-02-05T23:28:20.118911Z"},"trusted":true},"outputs":[],"source":["if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") != \"\":\n","    RUNNER = \"kaggle\"\n","elif \"google.colab\" in str(IPython.get_ipython()):\n","    RUNNER = \"colab\"\n","else:\n","    # assume running on a local machine\n","    RUNNER = \"local\""]},{"cell_type":"markdown","metadata":{"id":"8aWsRicMMXxH"},"source":["# Determining device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.12544Z","iopub.status.busy":"2022-02-05T23:28:20.124942Z","iopub.status.idle":"2022-02-05T23:28:20.172923Z","shell.execute_reply":"2022-02-05T23:28:20.172224Z","shell.execute_reply.started":"2022-02-05T23:28:20.125406Z"},"id":"XtZOpwh-MZvB","outputId":"2b14ded5-ee51-44de-c5a1-4f544842cd53","trusted":true},"outputs":[],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"K--tlf-bNvPJ"},"source":["# Setting globals"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.174495Z","iopub.status.busy":"2022-02-05T23:28:20.174165Z","iopub.status.idle":"2022-02-05T23:28:20.182991Z","shell.execute_reply":"2022-02-05T23:28:20.182225Z","shell.execute_reply.started":"2022-02-05T23:28:20.174459Z"},"id":"moKn5k7UOBsg","trusted":true},"outputs":[],"source":["BATCH_SIZE = 8\n","IMAGE_SIZE = 256\n","NORMALIZATION_PARAMS = {\"mean\": (0.485, 0.456, 0.406),\n","                        \"std\": (0.229, 0.224, 0.225)}\n","plt.rcParams[\"figure.figsize\"] = (15, 15)\n","USE_SMALLER_DATASET = False"]},{"cell_type":"markdown","metadata":{"id":"vCHZO0NlS9kS"},"source":["# Helper functions and classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.184785Z","iopub.status.busy":"2022-02-05T23:28:20.184355Z","iopub.status.idle":"2022-02-05T23:28:20.197725Z","shell.execute_reply":"2022-02-05T23:28:20.197101Z","shell.execute_reply.started":"2022-02-05T23:28:20.184639Z"},"id":"brHE-1DxTIlg","trusted":true},"outputs":[],"source":["class Denormalize():\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","    \n","    def __call__(self, tensor):\n","        for t, m ,s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","        return tensor\n","\n","class StyleTransferDataset(torch.utils.data.Dataset):\n","    def __init__(self, content_dir, style_dir, transforms=None):\n","        content_images = glob.glob(content_dir + \"/*\")\n","        style_images = glob.glob(style_dir + \"/*\")\n","        self.images = list(zip(content_images, style_images))\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        content, style = self.images[idx]\n","        content_img = skimage.io.imread(content)\n","        if len(content_img.shape) < 3:\n","            content_img = skimage.color.gray2rgb(content_img)\n","        style_img = skimage.io.imread(style)\n","        if len(style_img.shape) < 3:\n","            content_img = skimage.color.gray2rgb(content_img)\n","        content_img = torchvision.transforms.ToTensor()(content_img)\n","        style_img = torchvision.transforms.ToTensor()(style_img)\n","        if self.transforms:\n","            content_img, style_img = self.transforms(content_img), self.transforms(style_img)\n","        return content_img, style_img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.199989Z","iopub.status.busy":"2022-02-05T23:28:20.199578Z","iopub.status.idle":"2022-02-05T23:28:20.209376Z","shell.execute_reply":"2022-02-05T23:28:20.208528Z","shell.execute_reply.started":"2022-02-05T23:28:20.199954Z"},"id":"pm0u7GMPXpbq","trusted":true},"outputs":[],"source":["class AdaptiveInstanceNorm2d(torch.nn.Module):\n","    def __init__(self, eps=1e-6):\n","        super().__init__()\n","        self.eps = eps\n","\n","    def _get_mean(self, features):\n","        batch_size, c = features.size()[:2]\n","        features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n","        return features_mean\n","    \n","    def _get_std(self, features):\n","        batch_size, c = features.size()[:2]\n","        features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + self.eps\n","        return features_std\n","\n","    def forward(self, content, style):\n","        content_mean, content_std = self._get_mean(content), self._get_std(content)\n","        style_mean, style_std = self._get_mean(style), self._get_std(style)\n","        normalized = style_std * (content - content_mean) / content_std + style_mean\n","        return normalized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.211235Z","iopub.status.busy":"2022-02-05T23:28:20.2109Z","iopub.status.idle":"2022-02-05T23:28:20.2293Z","shell.execute_reply":"2022-02-05T23:28:20.228506Z","shell.execute_reply.started":"2022-02-05T23:28:20.211196Z"},"id":"5-Af7Ln7bn_K","trusted":true},"outputs":[],"source":["def fit_epoch(data_train, model, optimizer, criterion, epoch, epochs):\n","    model.train()\n","    running_loss = 0.0\n","    processed_data = 0\n","    styled = []\n","    for content, style in tqdm.tqdm(data_train, desc=f\"Fitting epoch {epoch + 1}/{epochs}\", unit=\"batch\", unit_scale=False):\n","        try:\n","            content, style = content.to(DEVICE), style.to(DEVICE)\n","            optimizer.zero_grad()\n","            output, t = model(content, style)\n","            output_features = model.encoder(output, output_last=True)\n","            content_middle = model.encoder(output, output_last=False)\n","            style_middle = model.encoder(style, output_last=False)\n","            loss = criterion(output_features, t, content_middle, style_middle)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * content.size(0)\n","            processed_data += content.size(0)\n","            styled.append(output.detach().cpu())\n","        finally:\n","            content, style = content.cpu(), style.cpu()\n","            del content, style\n","            torch.cuda.empty_cache()\n","    train_loss = running_loss / processed_data\n","    return torch.cat(styled, dim=0), train_loss\n","\n","def train_model(data_train, data_val, model, optimizer, criterion, epochs, start_epoch=0, checkpoint_cooldown=10):\n","    history = []\n","    prev_lr = optimizer.param_groups[0][\"lr\"]\n","    start_time = time.time()\n","    with tqdm.tqdm(desc=\"Epoch\", total=epochs, unit=\"epoch\", unit_scale=False) as pbar:\n","        for epoch in range(epochs):\n","            try:\n","                output, train_loss = fit_epoch(data_train, model, optimizer, criterion, epoch, epochs)\n","                IPython.display.clear_output(wait=True)\n","                history.append((train_loss, val_loss, optimizer.param_groups[0][\"lr\"]))\n","                show_pics_train(data_val, output, history[-1], epoch, 6)\n","                pbar.update(1)\n","                pbar.refresh()\n","\n","                if (epoch + 1) % checkpoint_cooldown == 0:\n","                    save_model(f\"nst_model_{epoch + 1}.tar\", mode=\"training\", model=model, optimizer=optimizer, loss=criterion, history=history, epoch=epoch)\n","            except KeyboardInterrupt as stop:\n","                tqdm.tqdm.write(f\"Training interrupted at epoch {epoch + 1}. Returning history\")\n","                return history\n","    end_time = time.time()\n","    train_time = end_time - start_time\n","    tqdm.tqdm.write(f\"Overall training time: {train_time: 0.1f} seconds\")\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.234003Z","iopub.status.busy":"2022-02-05T23:28:20.233263Z","iopub.status.idle":"2022-02-05T23:28:20.250908Z","shell.execute_reply":"2022-02-05T23:28:20.249664Z","shell.execute_reply.started":"2022-02-05T23:28:20.233965Z"},"id":"rdct7WZbdCki","trusted":true},"outputs":[],"source":["def show_pics_train(data_val, output, stats, epoch, sample_size):\n","    log_template = \"Styled images on epoch {ep: 03d}.\\n\\\n","    Train loss: {t_loss: 0.4f}\"\n","    content, style = next(iter(data_val))\n","    content = content[:sample_size]\n","    style = style[:sample_size]\n","    denorm = Denormalize(mean=NORMALIZATION_PARAMS[\"mean\"], std=NORMALIZATION_PARAMS[\"std\"])\n","    styled = denorm(output[:sample_size]).permute(0, 2, 3, 1)\n","    content = denorm(content).permute(0, 2, 3, 1)\n","    style = denorm(style).permute(0, 2, 3, 1)\n","    for i in range(sample_size):\n","        plt.subplot(3, sample_size, i + 1)\n","        plt.imshow(np.clip(content[i].squeeze().numpy(), 0, 1))\n","        plt.title(\"Content\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(3, sample_size, i + 1 + sample_size)\n","        plt.imshow(np.clip(style[i].squeeze().numpy(), 0, 1))\n","        plt.title(\"Style\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(3, sample_size, i + 1 + 2 * sample_size)\n","        plt.imshow(np.clip(styled[i].squeeze().numpy(), 0, 1))\n","        plt.title(\"Styled\")\n","        plt.axis(\"off\")\n","    plt.suptitle(log_template.format(ep=epoch + 1, t_loss=stats[0]))\n","    plt.show();\n","\n","def plot_pics(pics, sample_size=6):\n","    for i in range(sample_size):\n","        plt.subplot(2, sample_size // 2 + 1, i + 1)\n","        image = pics[i]\n","        image = image.permute(1, 2, 0)\n","        plt.imshow(image.squeeze().numpy())\n","        plt.title(\"Images\")\n","        plt.axis(\"off\")\n","    plt.show()\n","\n","def plot_loss(history):\n","    loss, _ = zip(*history)\n","    plt.figure(figsize=(15, 9))\n","    plt.plot(loss, label=\"Train loss\")\n","    plt.legend(loc=\"best\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","def plot_learn_rate(history):\n","    _, _, learn_rate = zip(*history)\n","    plt.figure(figsize=(15, 9))\n","    plt.plot(learn_rate, label=\"Learn rate\")\n","    plt.legend(loc=\"best\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Learn rate\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.253171Z","iopub.status.busy":"2022-02-05T23:28:20.252701Z","iopub.status.idle":"2022-02-05T23:28:20.262872Z","shell.execute_reply":"2022-02-05T23:28:20.262096Z","shell.execute_reply.started":"2022-02-05T23:28:20.253131Z"},"id":"P6GU1rxaxFtU","trusted":true},"outputs":[],"source":["def style_transfer(model, content, style):\n","    model.eval()\n","    try:\n","        content, style = content.to(DEVICE), style.to(DEVICE)\n","        if content.dim() != 4:\n","            content = content.unsqueeze(0)\n","        if style.dim() != 4:\n","            style = style.unsqueeze(0)\n","        with torch.no_grad():\n","            styled, _ = model(content, style)\n","        styled = styled.detach().cpu()\n","    finally:\n","        content, style = content.cpu(), style.cpu()\n","        del content, style\n","        torch.cuda.empty_cache()\n","    return styled"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.264908Z","iopub.status.busy":"2022-02-05T23:28:20.264602Z","iopub.status.idle":"2022-02-05T23:28:20.277838Z","shell.execute_reply":"2022-02-05T23:28:20.276976Z","shell.execute_reply.started":"2022-02-05T23:28:20.264871Z"},"id":"BW1LsS03xp-s","trusted":true},"outputs":[],"source":["def save_model(path, model, mode=\"inference\", **kwargs):\n","    if mode == \"training\":\n","        torch.save({\n","            \"epoch\": kwargs[\"epoch\"],\n","            \"model_state_dict\": model.state_dict(),\n","            \"optimizer_state_dict\": kwargs[\"optimizer\"].state_dict(),\n","            \"loss\": kwargs[\"loss\"],\n","            \"history\": kwargs[\"history\"]},\n","            path)\n","    else:\n","        torch.save(model.state_dict(), path)\n","\n","def load_model(path, model_arch, mode=\"inference\", optim_class=None, optim_kwargs=None):\n","    if mode == \"training\":\n","        if not optim_class:\n","            raise ValueError(\"Optimizer class required to load a model saved for training.\")\n","        model = model_arch()\n","        checkpoint = torch.load(path)\n","        model.load_state_dict(checkpoint[\"model_state_dict\"])\n","        if not optim_kwargs:\n","            optim = optim_class(model.parameters())\n","        else:\n","            optim = optim_class(model.parameters(), **optim_kwargs)\n","        optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n","        epoch = checkpoint[\"epoch\"]\n","        loss = checkpoint[\"loss\"]\n","        history = checkpoint[\"history\"]\n","        model.eval()\n","        return model, optim, epoch, loss, history\n","    else:\n","        # loading from TorchScript\n","        model = model_arch()\n","        model.load_state_dict(torch.load(path))\n","        model.eval()\n","        return model\n","\n","def save_history(path, history):\n","    with open(path, \"wb\") as f:\n","        pickle.dump(history, f)\n","\n","def load_history(path):\n","    with open(path, \"rb\") as f:\n","        history = pickle.load(f)\n","        \n","    return history"]},{"cell_type":"markdown","metadata":{"id":"okbAd8vi0d0y"},"source":["# Downloading data"]},{"cell_type":"markdown","metadata":{},"source":["There are different ways to download required data depending on runner type, and different requirements for those to work.\n","\n","If you are:\n","* Running the notebook in a Kaggle session: add the `shaorrran/coco-wikiart-nst-dataset-512-100000` dataset to the session.\n","* Running the notebook in Google Colab: upload your `kaggle.json` file into the session.\n","* Running the notebook locally: have Kaggle API installed and your token ready."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:28:20.279901Z","iopub.status.busy":"2022-02-05T23:28:20.279545Z","iopub.status.idle":"2022-02-05T23:28:20.317782Z","shell.execute_reply":"2022-02-05T23:28:20.316795Z","shell.execute_reply.started":"2022-02-05T23:28:20.279794Z"},"trusted":true},"outputs":[],"source":["DATASET_NAME = \"shaorrran/cocowikiart-nst-dataset-small\" if USE_SMALLER_DATASET \\\n","else \"shaorrran/coco-wikiart-nst-dataset-512-100000\"\n","if RUNNER == \"kaggle\":\n","    name = DATASET_NAME.split(\"/\")[1]\n","    DATASET_PATH = f\"/kaggle/input/{name}\"\n","elif RUNNER == \"colab\":\n","    from google.colab import files\n","    files.upload()\n","    os.makedirs(\"/root/.kaggle\", exist_ok=True)\n","    !mv kaggle.json /root/.kaggle/kaggle.json\n","    !chmod 600 /root/.kaggle/kaggle.json\n","    !kaggle datasets download -d {DATASET_NAME} --unzip\n","    DATASET_PATH = os.getcwd()\n","else:\n","    if not os.exists(\"content.zip\") or os.exists(\"style.zip\") \\\n","    and not (os.path.isdir(\"content\") and os.path.isdir(\"style\")):\n","        !kaggle datasets download -d {DATASET_NAME} --unzip\n","    else:\n","        with zipfile.ZipFile(\"content.zip\", \"r\") as archive:\n","            for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting\", unit=\"files\", unit_scale=False):\n","                archive.extract(member, os.getcwd())\n","        with zipfile.ZipFile(\"style.zip\", \"r\") as archive:\n","            for member in tqdm.tqdm(archive.namelist(), desc=\"Extracting\", unit=\"files\", unit_scale=False):\n","                archive.extract(member, os.getcwd())\n","        !rm content.zip\n","        !rm style.zip\n","    DATASET_PATH = os.getcwd()"]},{"cell_type":"markdown","metadata":{"id":"v6rcgpQjGLAV"},"source":["# Creating Dataset and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:29:41.594268Z","iopub.status.busy":"2022-02-05T23:29:41.593567Z","iopub.status.idle":"2022-02-05T23:29:44.910059Z","shell.execute_reply":"2022-02-05T23:29:44.9092Z","shell.execute_reply.started":"2022-02-05T23:29:41.594229Z"},"id":"VAYtGJ6IGKCG","trusted":true},"outputs":[],"source":["data = StyleTransferDataset(os.path.join(DATASET_PATH, \"content\"), os.path.join(DATASET_PATH, \"style\"), \n","                            transforms=torchvision.transforms.Compose([\n","                                torchvision.transforms.RandomCrop(IMAGE_SIZE),\n","                                torchvision.transforms.Normalize(\n","                                mean=NORMALIZATION_PARAMS[\"mean\"],\n","                                std=NORMALIZATION_PARAMS[\"std\"],\n","                            )]))\n","data_train, data_val = torch.utils.data.random_split(data, \n","                                                     [int(0.95 * len(data)), len(data) - int(0.95 * len(data))], \n","                                                     generator=torch.Generator().manual_seed(RANDOM_STATE))\n","train_loader = torch.utils.data.DataLoader(data_train, batch_size=int(BATCH_SIZE), shuffle=True, \n","                                           num_workers=torch.multiprocessing.cpu_count(), \n","                                           pin_memory=True, \n","                                           drop_last=True)\n","val_loader = torch.utils.data.DataLoader(data_val, batch_size=int(BATCH_SIZE), shuffle=False, \n","                                           num_workers=torch.multiprocessing.cpu_count(), \n","                                           pin_memory=True, \n","                                           drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"_kBHl_xKIMqs"},"source":["# Defining models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:10.107864Z","iopub.status.busy":"2022-02-05T23:30:10.107543Z","iopub.status.idle":"2022-02-05T23:30:10.132121Z","shell.execute_reply":"2022-02-05T23:30:10.131361Z","shell.execute_reply.started":"2022-02-05T23:30:10.107827Z"},"id":"VlbOZCbrIN7N","trusted":true},"outputs":[],"source":["class VGGEncoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        vgg = torchvision.models.vgg19(pretrained=True).features\n","        self.slice0 = vgg[:2]\n","        self.slice1 = vgg[2:7]\n","        self.slice2 = vgg[7:12]\n","        self.slice3 = vgg[12:21]\n","        for p in self.parameters():\n","            p.requires_grad = False\n","\n","    def to(self, device):\n","        # redefine _apply to move all params to one device\n","        new_self = super().to(device)\n","        for p in new_self.parameters():\n","            p = p.to(device)\n","        return new_self\n","        \n","    def forward(self, images, output_last=False):\n","        h0 = self.slice0(images)\n","        h1 = self.slice1(h0)\n","        h2 = self.slice2(h1)\n","        h3 = self.slice3(h2)\n","        if output_last:\n","            return h3\n","        return h0, h1, h2, h3\n","\n","class Decoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = torch.nn.Sequential(collections.OrderedDict([\n","            (\"conv0\", torch.nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act0\", torch.nn.ReLU(True)),\n","            (\"upsample0\", torch.nn.Upsample(scale_factor=2)),\n","            (\"conv1\", torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act1\", torch.nn.ReLU(True)),\n","            (\"conv2\", torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act2\", torch.nn.ReLU(True)),\n","            (\"conv3\", torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act3\", torch.nn.ReLU(True)),\n","            (\"conv4\", torch.nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act4\", torch.nn.ReLU(True)),\n","            (\"upsample1\", torch.nn.Upsample(scale_factor=2)),\n","            (\"conv5\", torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act5\", torch.nn.ReLU(True)),\n","            (\"conv6\", torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act6\", torch.nn.ReLU(True)),\n","            (\"upsample2\", torch.nn.Upsample(scale_factor=2)),\n","            (\"conv7\", torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act7\", torch.nn.ReLU(True)),\n","            (\"conv8\", torch.nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\")),\n","            (\"act8\", torch.nn.ReLU(True))\n","            ]))\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class StyleTransferCNN(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = VGGEncoder()\n","        self.decoder = Decoder()\n","        self.instnorm = AdaptiveInstanceNorm2d()\n","    \n","    def _apply(self, fn):\n","        # redefine to move encoder and decoder to the same device the main model is on\n","        super()._apply(fn)\n","        self.encoder._apply(fn)\n","        self.decoder._apply(fn)\n","        return self\n","\n","    def forward(self, content, style, alpha=1.0):\n","        content_features = self.encoder(content, output_last=True)\n","        style_features = self.encoder(style, output_last=True)\n","        t = alpha * self.instnorm(content_features, style_features) + (1 - alpha) * content_features\n","        return self.decoder(t), t"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:10.134206Z","iopub.status.busy":"2022-02-05T23:30:10.133933Z","iopub.status.idle":"2022-02-05T23:30:26.438432Z","shell.execute_reply":"2022-02-05T23:30:26.437699Z","shell.execute_reply.started":"2022-02-05T23:30:10.134163Z"},"id":"8J1UI5t-NtGB","trusted":true},"outputs":[],"source":["model = StyleTransferCNN().to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:26.439964Z","iopub.status.busy":"2022-02-05T23:30:26.439709Z","iopub.status.idle":"2022-02-05T23:30:31.303014Z","shell.execute_reply":"2022-02-05T23:30:31.302327Z","shell.execute_reply.started":"2022-02-05T23:30:26.439932Z"},"id":"mZw6gccENyqW","outputId":"42707633-95cc-4731-bdd2-8f1fbfd28aee","trusted":true},"outputs":[],"source":["torchinfo.summary(model, ((int(BATCH_SIZE), 3, IMAGE_SIZE, IMAGE_SIZE), (int(BATCH_SIZE), 3, IMAGE_SIZE, IMAGE_SIZE)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:31.305576Z","iopub.status.busy":"2022-02-05T23:30:31.305113Z","iopub.status.idle":"2022-02-05T23:30:32.0634Z","shell.execute_reply":"2022-02-05T23:30:32.06249Z","shell.execute_reply.started":"2022-02-05T23:30:31.30554Z"},"id":"gqDpk_ZsN_Aj","outputId":"cd70207b-35b3-42f4-8760-7931dfe74db1","trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"Jeab14LGOASp"},"source":["# Defining loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:32.06549Z","iopub.status.busy":"2022-02-05T23:30:32.065187Z","iopub.status.idle":"2022-02-05T23:30:33.191455Z","shell.execute_reply":"2022-02-05T23:30:33.19068Z","shell.execute_reply.started":"2022-02-05T23:30:32.065455Z"},"id":"soQqtiY4OBzQ","trusted":true},"outputs":[],"source":["class StyleTransferLoss(torch.nn.Module):\n","    def __init__(self, lam=10):\n","        super().__init__()\n","        self.lam = lam\n","\n","    def _style_loss(self, content_middle, style_middle):\n","        loss = 0\n","        inst_norm = AdaptiveInstanceNorm2d()\n","        for c, s in zip(content_middle, style_middle):\n","            c_mean, c_std = inst_norm._get_mean(c), inst_norm._get_std(c)\n","            s_mean, s_std = inst_norm._get_mean(s), inst_norm._get_std(s)\n","            loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)\n","        return loss\n","\n","    def _content_loss(self, content, t):\n","        return F.mse_loss(content, t)\n","\n","    def forward(self, content, t, content_middle, style_middle):\n","        return self._content_loss(content, t) + self.lam * self._style_loss(content_middle, style_middle)"]},{"cell_type":"markdown","metadata":{"id":"SZEn1pE3QHCR"},"source":["# Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:33.195701Z","iopub.status.busy":"2022-02-05T23:30:33.195208Z","iopub.status.idle":"2022-02-05T23:30:33.202635Z","shell.execute_reply":"2022-02-05T23:30:33.201968Z","shell.execute_reply.started":"2022-02-05T23:30:33.19566Z"},"id":"woAUbEidQH5o","trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","criterion = StyleTransferLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-05T23:30:33.205511Z","iopub.status.busy":"2022-02-05T23:30:33.205318Z"},"id":"gvvBnUCbQUhh","outputId":"ba0f6cd5-ea2a-4216-8006-d2449cd5591f","trusted":true},"outputs":[],"source":["max_epochs = 25\n","history_nst = train_model(train_loader, val_loader, model, optimizer, criterion, max_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elQ-W5hLndcV","outputId":"1aac8b34-d3e8-4c52-e33d-43a3167f889b","trusted":true},"outputs":[],"source":["plot_loss(history_nst)"]},{"cell_type":"markdown","metadata":{"id":"y4aH3RNgnkgF"},"source":["# Validating model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CQsnW7xnpqU","outputId":"116ca5c9-f32a-48d5-8b15-9395a3f7901b","trusted":true},"outputs":[],"source":["test_content, test_style = next(iter(val_loader))\n","test_styled = style_transfer(model, test_content, test_style)\n","plot_pics(Denormalize(mean=NORMALIZATION_PARAMS[\"mean\"], std=NORMALIZATION_PARAMS[\"std\"])(test_styled))"]},{"cell_type":"markdown","metadata":{"id":"AYtSSwoOoXwO"},"source":["# Saving model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TR8PzCjsoYdz","trusted":true},"outputs":[],"source":["save_model(f\"nst_vgg+adaptive_instnorm.pth\", model, mode=\"inference\")"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"markdown","metadata":{},"source":["Retraining model on a smaller dataset with 25 epochs yields only blurred blots instead of images... guess I have to look for something else."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
